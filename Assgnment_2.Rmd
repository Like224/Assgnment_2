---
title: 'Assignment #2: Meta-analysis of Ocean Acidification Effects on Behaviour'
author: "(Rick) Zirui Zhang u7457512"
date: "20/10/2022"
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

# **Statistical Analysis**

### 1.Install packages

```{r}
#Use  p_load function in the pacman to install and library a load of packages that we'll use.
library(pacman)
p_load(bookdown, tidyverse, ggforce, flextable, latex2exp, magick, plotrix, readxl, janitor, devtools,dplyr, gridExtra, cowplot, rstatix, GGally, metafor, DIZutils, png, MASS, emmeans, R.rsp,orchaRd, meta)
```

### 2.Import clark data into R, data Wrangling and generate the summary statistics for each of the fish species' average activity for each treatment.

```{r}
#import clark data into R
path <- "OA_activitydat_20190302_BIOL3207.csv"
data_c <- read_csv(path)
#import metadata data into R
path_m <- "ocean_meta_data.csv"
meta_data <- read_csv(path_m)
#import paperdata into R
path_c <- "clark_paper_data.csv"
paper_data <- read_csv(path_c)
#Observe the structure of the  data
head(meta_data)
head(paper_data)
```

```{r}
#In comparison, the summary statistics for each of the fish species’ average activity for each treatment in meta_data are in one column, and we need to get the summary statistics data and divide them into different columns like the form of meta_data

#omit all rows with NA values in any column 
data_c <- na.omit(data_c)
# Drop irrelevant columns
new_data_c <- subset(data_c, select = -c(comment, loc))
# Check spelling in species and treatment 
unique(new_data_c$species)
unique(new_data_c$treatment)
```

```{r}
#seems no spelling error
#Generate a summary table of statistics (means, SD, N) for each of the fish species’ average activity for each treatment and divide them into different columns like the form of meta_data
mat <-   summarise_at(group_by(new_data_c,species,treatment),vars(activity),funs(mean(.,),sd(.,))) %>%
  pivot_wider(names_from = treatment, values_from = c(mean,sd))
id <- summarise_at(group_by(new_data_c,species,treatment),vars(animal_id),funs(length(unique(.,)))) %>%
  pivot_wider(names_from = treatment, values_from = c(animal_id))
#Combining these two sets of data together
data_2 <- merge(mat,id)
#Modify the column name of data_3 to make it the same as in the metadata
colnames(data_2) <- c("Species", "oa.mean","ctrl.mean","oa.sd","ctrl.sd","oa.n","ctrl.n")
data_2
```

### 2.Merge the summary statistics generated from 1) with the metadata.

```{r}
data_3 <- cbind(paper_data,data_2)
data_3
```

### 3.Merge the combined summary statistics and metadata into the larger meta-analysis dataset

```{r}
# use rbind to combine
data_f <- rbind(data_3,meta_data)
```

### 4.Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor's escalc() function.

```{r}
#Calculate Log Response Ratio. where m1i and m2i are the observed means of the two groups, sd1i and sd2i are the observed standard deviations, and n1i and n2i denote the number of individuals in each group. Use the var.names argument to rename the effect size and sampling variance to “lnRR” and “V_lnRR”.
rom_data <- metafor::escalc(measure = "ROM", m1i = oa.mean, m2i = ctrl.mean, sd1i = oa.sd, sd2i = ctrl.sd, n1i = oa.n, n2i = ctrl.n, data = data_f,
    var.names = c("lnRR", "V_lnRR"))
##remove NAs
rom_data<-na.omit(rom_data)
```

### 5.Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor's rma.mv() function.

```{r}
# Add observation level variable to rom_data.
rom_data <- rom_data %>% mutate(residual = 1:n())  
# Meta-analytic model include a random effect of study and observation.
MLMA <- metafor::rma.mv(lnRR ~ 1, V = V_lnRR, random = list( ~1| Study, ~1 | residual), data=rom_data)
MLMA

```

### 6.Findings and interpretation of the above results

#### Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals).

##### Overall meta-analytic mean

```{r}
#We can see the overall meta-analytic mean from the model by extracting the intercept.
coef(MLMA)
```

It is estimated to be `r coef(MLMA)`, which tells us that the mean lnRR value is positive, but there is a rather weak overall association between ocean acidification and fish behavior change when we pool across all studies.

##### Measures of uncertainty around the mean estimate (95% confidence intervals).

```{r}
str(MLMA$ci.lb)
str(MLMA$ci.ub)
```

We can extract the 95% confidence intervals which range from `r MLMA$ci.lb` to `r MLMA$ci.ub`. 95% of the time we would expect the true mean to fall between lnRR values of \``r MLMA$ci.lb` to `r MLMA$ci.ub`.And if we were to repeat the experiment many times, 95% of the confidence intervals constructed would contain the true meta-analytic mean.

##### Measures of heterogeneity in effect size estimates across studies

```{r,tab.cap = "Total effect size hetereogneity, as well as the proportion of hetereogeneity in effects resulting from Study and Observational" }
# Calculate I2
i2_vals <- orchaRd::i2_ml(MLMA)

i2 <- tibble(type = firstup(gsub("I2_", "",names(i2_vals))), I2 = i2_vals)
flextable(i2) %>%
    align(part = "header", align = "center") %>%
    compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>%
    compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")),
        as_b("(%)")))
# Calculate the prediction intervals
pis <- predict(MLMA)
pis
```

Overall, the $I_{Total}^2$ is `r i2[1, "I2"]`,which means we have highly heterogeneous effect size data because At an accuracy of five decimal places, sampling changes barely affect the change in total effect. From the multilevel meta-analytic model we found that only `r i2[2, "I2"]` of the total variation in effect size estimates was due to between-study variation.

Our 95% prediction intervals are wide. Effect sizes (lnRR) are expected to range from `r  pis$pi.lb` to `r pis$pi.ub` 95% of the time with repeated experiments, suggesting a lot of inconsistency between studies.

##### Forest plot showing the mean estimate, 95% confidence interval, and prediction interval

```{r fig.cap= "Orchard plot showing the mean estimate, 95% confidence interval, and prediction interval, k = the number of effect sizes and the number of studies are in brackets. The hollow dots in the middle represent mean estimate，bold lines represent  95% confidence interval, thin lines represent prediction interval, The blue circles represent individual effect sizes"}
#use orchard plots (Improved forest plot) to show the mean estimate, 95% confidence interval, and prediction interval

orchaRd::orchard_plot(MLMA, mod = "1",  group = "Study" , data = rom_data,
    xlab = "log response ratio (lnRR)", trunk.size= 0.5, branch.size = 5, twig.size = 2, angle = 45)
```

### 7.Funnel plot for visually assessing the possibility of publication bias.

```{r funnel, fig.cap= "Funnel plot depicting elevated CO2 relative to some control on fish behaviour as a function of precision (1 / SE). The dotted lines are the theoretical 95% sampling variance intervals - the interval with which we expect effect size estimates to fall within if only sampling variance drives differences in effects. Shaded regions represent the p-value of studies. The white region indicates studies where the p-value is between 0.1 and 1; dark gray where the p-value of studies is between 0.05 and 0.1 and the lighter gray regions where the p-value of studies is significant."}
#Make a funnel plot to visualize the data in relation to the precision, inverse sampling standard error.
metafor::funnel(x = rom_data$lnRR, vi= rom_data$V_lnRR, yaxis = "seinv", xlab = "Log Response Ratio (lnRR)",level = c(0.1,0.05,0.01) ,shade = c("white", "gray55", "gray 75"),
    las = 1, ylim = c(0.1, 20), xlim = c(-4, 4),  legend = TRUE)
```



```{r po2, fig.cap= "Plot of lnRR against sampling variance for lnRR. A linear model was fit to the data."}
#Fitting a Multilevel Meta-Regression model to Test and Correct for Publication bias
ggplot(rom_data, aes(y = lnRR, x= V_lnRR,)) + geom_point() + geom_smooth(method = lm) + labs(y = "Log Response Ratio (lnRR)", x = "Sampling Variance of lnRR") + 
    theme_classic()
```
If we do not filter the sampling Variance of lnRR, we find two extremely large value of variance that significantly pulls down the slope, and prevented us from analyzing the distribution of the sample.

```{r po, fig.cap= "Plot of lnRR against sampling variance for lnRR. A linear model was fit to the data.(X-axis is filtered)"}
# filter the sampling Variance of lnRR，
po_rom_data <- rom_data %>% filter(V_lnRR < 1.0e+04)
ggplot(po_rom_data, aes(y = lnRR, x= V_lnRR,)) + geom_point() + geom_smooth(method = lm) + labs(y = "Log Response Ratio (lnRR)", x = "Sampling Variance of lnRR") +
    theme_classic()
```


### 8.Time-lag plot assessing how effect sizes may or may not have changed through time.

```{r Tl, fig.cap= "Plot of lnRR as a function of Print year. Points are scaled in relation to their precision (1/sqrt(V_lnRR)). Small points indicate effects with low precision or high sampling varaince"}
ggplot(rom_data, aes(y = lnRR, x = Year..print., size = 1/sqrt(V_lnRR))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Print Year",
    y = "Log Response Ratio (lnRR)", size = "Precision (1/SE)") +
    theme_classic()
```

In this plot, we can see that there does seem to be a clear negative correlation between lnRR and year, but because there are several lnRR values that are too small , which produce the big point.To see the variation between precision, we filter these values out

```{r TL2, fig.cap= "Plot of lnRR as a function of Print year and filter out the too-small V_lnRR,. Points are scaled in relation to their precision (1/sqrt(V_lnRR)). Small points indicate effects with low precision or high sampling varaince"}
Tl_rom_data <- rom_data %>% filter(V_lnRR > 1.0e-05)
ggplot(Tl_rom_data, aes(y = lnRR, x = Year..print., size = sqrt(1/V_lnRR))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Log Response Ratio (lnRR)", size = "Precision (1/SE)") +
    theme_classic()
```


### 9.Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias

```{r}
# Including year as moderator
MLMY <- metafor::rma.mv(lnRR ~ Year..print., V = V_lnRR, random = list( ~1| Study, ~1 | residual), data=rom_data)
MLMY
r2_time <- orchaRd::r2_ml(MLMY)
r2_time
```

### 10.Formal meta-regression model that includes inverse sampling variance to test for file-drawer biases

```{r}
MLMI <- metafor::rma.mv(lnRR ~ (1/V_lnRR), V = V_lnRR, random = list( ~1| Study, ~1 | residual), data=rom_data)
MLMI
r2_A <- orchaRd::r2_ml(MLMI)  
r2_A
```


### 11.A paragraph that discusses the potential for publication bias based on the meta-regression results.

We can see from Fig. \@ref(fig:funnel) This is a typical funnel shape. Most effects are present in the negative space. There are a lot of studies in the light grey regions that the p-value was significant. There is a clear gap in the lower right corner for positive lnRR based on very small sample sizes. Interestingly, for negative LnRR, the research of a large amount of small samples was published.However, most of these studies have large p-values.

We can see from Fig. \@ref(fig:po) Here is a significant negative slope. This is because effect sizes are unevenly distributed on both sides of the funnel plot, and when the sampling variance is large, the average effect size is shifted, resulting in a slope.We can see from the linear model fit that when the sampling variance is large, the average effect size is dragged downwards.  Another point worth noting is that if we do not filter the sampling Variance of lnRR, we find two extremely large value of variance that significantly pulls down the slope.(Fig. \@ref(fig:po2)), which has a great impact to the slope and distribution of effect sizes.

Based on the above two points, we can get Publication bias is reflected here because there are few effects in the opposite direction, and when the sample size is small, our hypothesis does not predict that direction, so the sampling variance is large. It shows these studies are difficult to publish, or that the authors are unlikely to believe them and therefore do not publish them.

We can see from Fig. \@ref(fig:TL2) The impact of ocean acidification studies on fish behaviour has declined. In this plot, we can see that there does seem to be a clear negative correlation between lnRR and year. Effect size magnitudes (lnRR) in this area have been declining over the past decade, with a large number of effect size magnitudes >5 in 2010-2014 and mostly <3 after 2015. average effect size magnitudes in early studies were too large, hovering at moderate effect sizes from 2012 to 2014 and almost disappeared in recent years.These early studies appear to have a far higher effect size compared with studies that are done in later years.In the early studies, the large effects of the great effect of acidification on  fish behavior have not appeared in most of the past five years.

Based on the above two points, we can get Two other publication bias maybe reflected here. First, Methodological biases. Experiments with small sample sizes are more prone to statistical error, and studies with large sample sizes should be more trustworthy than those with small sample sizes, but in the data used for this assignment, almost all studies with maximum effect sizes had relatively low average sample sizes (below 30 fish). Second, Publication and citation bias. At the beginning of this field, the results of showing strong effects are often easier to publish than research that displayed weaker or invalid results, and published on magazines that have a greater impact. The author and journal articles with optional publications may help the spread of the research and continuous existence of the strong effect of this field. Even if these effects may not be true due to low samples or other reasons.

Time-lag explains `r r2_A[1]` of the variation in lnRR. Since it is small, it is not reflected we have evidence of a time-lag bias.

### 12.Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

#### Identify studies contributing to publication bias.

We can see from Fig. \@ref(fig:TL2). A large number of studies prior to 2012 had effect sizes >5, and after 2015 were mostly <3. The average effect size was too large for earlier studies, which appeared to have higher effect sizes than studies done later in life. In earlier studies, large effects on acidification and fish behavior were absent for most of the past five years. Therefore, it is possible that these studies have caused publication bias, and we will delete the data before 2012.

```{r IS, fig.cap= "The impact of marine acidization research on the decline in fish behavior (not included in the study before 2011.)"}
IS_rom_data <- rom_data %>% filter( Year..print.> 2011)
ggplot(IS_rom_data, aes(y = lnRR, x = Year..print., size = sqrt(1/V_lnRR))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Log Response Ratio (lnRR)", size = "Precision (1/SE)") +
    theme_classic()
```

We can see from Fig. \@ref(fig:IS). After removing studies before 2012, the decline effect from 2012 to 2019 is no longer significant, from which we can conclude that studies with higher effect size before 2012 may have contributed to some of the publication bias.

We can see from Fig. \@ref(fig:po2). If we do not filter the sampling Variance of lnRR, we find two extremely large value of variance that significantly pulls down the slope, which has a great impact to the slope and distribution of effect sizes.We can conclude that studies with extremely large value of variance may have contributed to some of the publication bias.

#### My meta-analysis results compare with a meta-analysis by Clement et. al. (2022)

From the results point of view, the results of these two meta-analysis are roughly the same. All detected the strong decline effect, also proposed several possibilities: Early studies have small samples and selective publishing bias. 

Apart from that, They looked at the impact of variables such as average citations, total citations since 2020 and journal impact factor on the data. More strongly proved Publication and citation bias. Results showing strong effects were generally more likely to be published in higher impact journals than studies showing weak or invalid results. They found that the most prominent effects of ocean acidification on fish behavior were published in journals with high impact factors. In addition, these studies all had high citation frequencies, indicating a stronger impact on the field than others.

However, the biggest difference between the two analyses is that meta-analysis by Clement et. al. (2022)'s effect sizes were transformed to the absolute value of lnRR.
Such a conversion provides only a measure of effect size. Thus, absolute effect sizes would be overestimated, but could still be used to test for a decreasing effect of effect size magnitude over time. Note, however, that by ignoring directional effect sizes, I think this makes it difficult to make true population-level inferences.

#### Concerns about these studies

The results suggest that the large effects of ocean acidification on fish behaviour that emerged in earlier studies have been mostly absent in the last five years. Since the birth of the field, studies of ocean acidification on fish behavior have been characterized by declining effects. 

The bad news is that the vast majority of early studies with large effects in this field were characterized by low sample sizes, but were published in high-impact journals that have had a significant impact on the field in terms of citation rates and have even influenced the course of research in this area of science. Indeed, a recent analysis suggests that there is a publication bias in field studies of the biology of global change, which contributes to the proliferation of underpowered studies that report excessive effect sizes [1].

The good news is that the impact of the earlier studies is declining year by year and the effect size of the field is levelling off. But it also requires us to reflect on Whether this decline effect is common in the field of ecology and even the entire biology (Similar results are also reported in other areas of ecology and evolution. Maybe the most noteworthy research on land plants to react to high carbon dioxide) [2], as well as its impact on our scientific development process. Whether we can then improve the methodology in future studies to minimize the possibility of declining effects.

# **Reproducibility**

### Github

[My GitHub Repository](https://github.com/Like224/Assgnment_2)

### Reference
1. Yang Y, Hillebrand H, Malgorzata L, Cleasby I, Nakagawa S. Low statistical power and overestimated anthropogenic impacts, exacerbated by publication bias, dominate field studies in global change biol- ogy. EcoEvoRxiv. 2021.

2. Murtaugh PA. Journal quality, effect size, and publication bias in meta-analysis. Ecology. 2002; 83:1162–6.